{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Captioning\n",
    "\n",
    "For this exercise, we are going to explore video captioning. We will apply the same approach used in [1]. This technique is similar to what we experimented with in the image captioning example. A feature vector and caption are concatenated to form the input for the RNN. For this, the mean of the fc7 feature vector from frames in each video clip is used. The [Microsoft Research Video Description Corpus\n",
    "(MSVD)](https://www.microsoft.com/en-us/download/details.aspx?id=52422)[2] is used for this experiment. This data set contains roughly 2000 video clips with more than ten captions for each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import inspect\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "#import reader\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "import sys\n",
    "sys.path.insert(0, '/data/models/slim')\n",
    "\n",
    "slim=tf.contrib.slim\n",
    "from nets import vgg\n",
    "\n",
    "from preprocessing import vgg_preprocessing\n",
    "\n",
    "%matplotlib inline  \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSVD Data  \n",
    "Let's take a look at our data. First, we will evaluate a single image, its captions, and mean feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_file = open('MSVDCaptions_lower_nochars.txt', 'r') \n",
    "data=caption_file.readlines()\n",
    "caption_file.close()\n",
    "CaptionsandMovies=[row.split(', ') for row in data]\n",
    "print(CaptionsandMovies[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "ListofClips = glob('/data/msvd/frames_224/*')\n",
    "print(ListofClips[0])#[22:])\n",
    "clip_frames = [f for f  in os.listdir(ListofClips[0])]\n",
    "print(len(clip_frames),' frames from this clip are used to train the network')\n",
    "print('One Frame from clip ',ListofClips[0][22:],' is displayed below')\n",
    "one_image=ndimage.imread(ListofClips[0]+'/'+clip_frames[0])\n",
    "#resize for vgg network\n",
    "resize_img=misc.imresize(one_image,[224,224])\n",
    "\n",
    "#Show image\n",
    "plt.imshow(resize_img)\n",
    "\n",
    "#Display Captions for Video clips\n",
    "MovieCaption=[vid_id[1] for vid_id in CaptionsandMovies if vid_id[0]==ListofClips[0][22:]]\n",
    "print('captions for this clip')\n",
    "print(MovieCaption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a mean vector of a single video clip \n",
    "\n",
    "TRAIN_DATA_PATH='/data/msvd/'\n",
    "## Read Training files\n",
    "CLIPOFINTEREST='KpmVL4ANieA_0_9'\n",
    "one_clip_of_interest = [f for f  in os.listdir(TRAIN_DATA_PATH+'frames_224/'+CLIPOFINTEREST)]\n",
    "clip_feature_vectors={}\n",
    "for frame in one_clip_of_interest:   \n",
    "    tf.reset_default_graph()   \n",
    "    one_image=ndimage.imread('/data/msvd/frames_224/'+CLIPOFINTEREST+'/'+frame)\n",
    "    #resize for vgg network\n",
    "    resize_img=misc.imresize(one_image,[224,224])\n",
    "    if len(one_image.shape)!= 3: #Check to see if the image is grayscale if True mirror colorband\n",
    "        resize_img=np.asarray(np.dstack((resize_img, resize_img, resize_img)), dtype=np.uint8)\n",
    "    #image_size = vgg.vgg_16.default_image_size\n",
    "    processed_image = vgg_preprocessing.preprocess_image(resize_img, 224, 224, is_training=False)\n",
    "    processed_images  = tf.expand_dims(processed_image, 0)      \n",
    "    network,endpts= vgg.vgg_16(processed_images, is_training=False)\n",
    "   \n",
    "    init_fn = slim.assign_from_checkpoint_fn(os.path.join('/data/mscoco/vgg_16.ckpt'),slim.get_model_variables('vgg_16'))\n",
    "    sess = tf.Session()\n",
    "    init_fn(sess)\n",
    "    NETWORK,ENDPTS=sess.run([network,endpts])\n",
    "    sess.close()\n",
    "    clip_feature_vectors[frame]=ENDPTS['vgg_16/fc7'][0][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_feature_vectors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numframes=len(clip_feature_vectors.keys())\n",
    "meanfc7vector=np.zeros([numframes,4096])\n",
    "for i in range(numframes):\n",
    "    i=i+1\n",
    "    meanfc7vector[i-1,:]=clip_feature_vectors['frame_'+'%06d' %i+'_224.bmp'].reshape(1,4096)\n",
    "\n",
    "meanfc7vector=np.mean(meanfc7vector,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(meanfc7vector)\n",
    "plt.xlabel('feature vector index')\n",
    "plt.ylabel('amplitude')\n",
    "plt.title('mean fc7 feature vector')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A npy file of all of the mean feature vectors has been created\n",
    "file_loader=np.load('/data/msvd/train_msvd_mean_fc7.npy').tolist()\n",
    "print('Number of videos in the training set: ', len(file_loader.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to align the feature maps with their corresponding captions. A npy file comprised of the mean feature vectors has been created. We will now combine these feature maps with their respective captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 3 lists image_id, feature maps, and captions.\n",
    "video_id_key=[]\n",
    "feature_maps_to_id=[]\n",
    "caption_to_id=[]\n",
    "for observed_vid in file_loader.keys():   \n",
    "    for k in range(len(CaptionsandMovies)):\n",
    "        if CaptionsandMovies[k][0]==observed_vid:\n",
    "            video_id_key.append([observed_vid])\n",
    "            feature_maps_to_id.append(file_loader[observed_vid])\n",
    "            caption_to_id.append(CaptionsandMovies[k][1])\n",
    "  \n",
    "\n",
    "print('number of captions  and datapoints',len(caption_to_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poc=50001\n",
    "print(video_id_key[poc])\n",
    "print(feature_maps_to_id[poc])\n",
    "print(caption_to_id[poc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will encode our captions and set a dictionary limit\n",
    "\n",
    "num_steps=20\n",
    "######################################################################\n",
    "##Create a list of all of the sentences.\n",
    "DatasetWordList=[]\n",
    "for dataset_caption in caption_to_id:\n",
    "        DatasetWordList+=dataset_caption.split()\n",
    "\n",
    "#Determine number of distint words \n",
    "distintwords=collections.Counter(DatasetWordList)\n",
    "#Order words \n",
    "count_pairs = sorted(distintwords.items(), key=lambda x: (-x[1], x[0])) #ascending order\n",
    "words, occurence = list(zip(*count_pairs))\n",
    "DictionaryLength=occurence.index(4) #index for words that occur 4 times or less\n",
    "words=['PAD','UNK','EOS']+list(words[:DictionaryLength])\n",
    "word_to_id=dict(zip(words, range(len(words))))\n",
    "#####################  Tokenize Sentence #######################\n",
    "Tokenized=[]\n",
    "for full_words in caption_to_id:\n",
    "        EmbeddedSentence=[word_to_id[word] for word in full_words.split() if word in word_to_id]+[word_to_id['EOS']]\n",
    "        #Pad sentences that are shorter than the number of steps \n",
    "        if len(EmbeddedSentence)<num_steps:\n",
    "            b=[word_to_id['PAD']]*num_steps\n",
    "            b[:len(EmbeddedSentence)]=EmbeddedSentence\n",
    "        if len(EmbeddedSentence)>num_steps:\n",
    "            b=EmbeddedSentence[:num_steps]\n",
    "        if len(b)==EmbeddedSentence:\n",
    "            b=EmeddedSentence\n",
    "        b=[word_to_id['UNK'] if x>=DictionaryLength else x for x in b] #turn all words used 4 times or less to 'UNK'\n",
    "        #print(b)\n",
    "        Tokenized+=[b]\n",
    "        \n",
    "print(\"Number of words in this dictionary \", len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cell are used to configure the RNN and train it to create captions for the video clips. To save time a pretrained network is provided. Please load it and finetune. The pretrained network is saved here /data/msvd/video_4096_6model_iter99999\n",
    "\n",
    "We are going to use the same function we used in the Image Captioning exercise to load our model.\n",
    "\n",
    "    saver.restore(sess, PRETRAINED_MODEL)\n",
    "\n",
    "What is the loss?\n",
    "   \n",
    "\n",
    "#### Troubleshooting assistance\n",
    "Sometimes between training tests error messages like this occur.\n",
    "\n",
    "#####  \"InternalError: Dst tensor is not initialized\"  \n",
    "\n",
    "##### \"ResourceExhaustedError: OOM when allocating tensor with shape\"\n",
    "\n",
    "If you receive this you may need to restart your Python kernel, which can be done by going to the Menu bar at the top of this Juptyer Notebook and selecting Kernel then Restart. Remember when restarting the Python kernel, all of your variables will need to be redefined. This will require that you reload the data and tokenize your sentences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_queue(TrainingInputs,FeatureVectors):\n",
    "    train_input_queue = tf.train.slice_input_producer(\n",
    "                    [TrainingInputs, np.asarray(FeatureVectors)],num_epochs=10000,\n",
    "                                    shuffle=True) #False before\n",
    "\n",
    "    ##Set our train data and label input shape for the queue\n",
    "\n",
    "    TrainingInputs=train_input_queue[0]\n",
    "    FeatureVectors=train_input_queue[1]\n",
    "    TrainingInputs.set_shape([num_steps])\n",
    "    FeatureVectors.set_shape([len(feature_maps_to_id[0])]) #fc7 is 4096\n",
    "    ##LabelInput.set_shape([num_steps])\n",
    "    min_after_dequeue=1000000\n",
    "    capacity = min_after_dequeue + 3 * batch_size \n",
    "    #input_x, target_y\n",
    "    tokenized_caption, input_feature_map = tf.train.batch([TrainingInputs, FeatureVectors],\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 capacity=capacity,\n",
    "                                                 num_threads=6)\n",
    "    return tokenized_caption, input_feature_map\n",
    "\n",
    "def rnn_model(Xconcat,input_keep_prob,output_keep_prob,num_layers,num_hidden):\n",
    "#Create a multilayer RNN\n",
    "#reuse=False for training but reuse=True for sharing\n",
    "    layer_cell=[]\n",
    "    for _ in range(num_layers):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_hidden, state_is_tuple=True)\n",
    "        lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell,\n",
    "                                          input_keep_prob=input_keep_prob,\n",
    "                                          output_keep_prob=output_keep_prob)\n",
    "        layer_cell.append(lstm_cell)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(layer_cell, state_is_tuple=True)\n",
    "    outputs, last_states = tf.contrib.rnn.static_rnn(\n",
    "        cell=cell,\n",
    "        dtype=tf.float32,\n",
    "        inputs=tf.unstack(Xconcat))\n",
    "\n",
    "    output_reshape=tf.reshape(outputs, [batch_size*(num_steps),num_hidden]) #[12==batch_size*num_steps,num_hidden==12]\n",
    "    pred=tf.matmul(output_reshape, variables_dict[\"weights_mscoco\"]) +variables_dict[\"biases_mscoco\"]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#######################################################################################################\n",
    "# Parameters\n",
    "num_hidden=4096\n",
    "num_steps=num_steps\n",
    "dict_length=len(words)\n",
    "batch_size=4\n",
    "num_layers=2\n",
    "loss_msvd=[]\n",
    "train_lr=0.00001\n",
    "#######################################################################################################\n",
    "TrainingInputs=Tokenized\n",
    "FeatureVectors=feature_maps_to_id\n",
    "\n",
    "tokenized_caption, input_feature_map=data_queue(TrainingInputs,FeatureVectors)\n",
    "## Make Variables    \n",
    "\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "#tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "variables_dict = {\n",
    "    \"weights_mscoco\":tf.Variable(tf.truncated_normal([num_hidden,dict_length],\n",
    "                                                     stddev=1.0,dtype=tf.float32),name=\"weights_mscoco\"),\n",
    "    \"biases_mscoco\": tf.Variable(tf.truncated_normal([dict_length],\n",
    "                                                     stddev=1.0,dtype=tf.float32), name=\"biases_mscoco\")}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TrainInput=tf.constant(word_to_id['PAD'],shape=[batch_size,1],dtype=tf.int32)\n",
    "#Pad the beginning of our caption. The first step now only has the image feature vector. Drop the last time step \n",
    "#to timesteps to 20\n",
    "TrainInput=tf.concat([tf.constant(word_to_id['PAD'],shape=[batch_size,1],dtype=tf.int32),\n",
    "                      tokenized_caption],1)[:,:-1]\n",
    "X_one_hot=tf.nn.embedding_lookup(np.identity(dict_length), TrainInput) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "#ImageFeatureTensor=input_feature_map\n",
    "Xconcat=tf.concat([input_feature_map+tf.zeros([num_steps,batch_size,4096]), \n",
    "                                                 tf.unstack(tf.to_float(X_one_hot),num_steps,1)],2)#[:num_steps,:,:]\n",
    "\n",
    "\n",
    "#the full caption is now the target sentence\n",
    "y_one_hot=tf.unstack(tf.nn.embedding_lookup(np.identity(dict_length), tokenized_caption),num_steps,1) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "\n",
    "y_target_reshape=tf.reshape(y_one_hot,[batch_size*num_steps,dict_length])\n",
    "pred=rnn_model(Xconcat,1.0,1.0,num_layers,num_hidden)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_target_reshape))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr,epsilon=1.0)\n",
    "\n",
    "\n",
    "gvs = optimizer.compute_gradients(cost,  aggregation_method = tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -10., 10.), var) for grad, var in gvs]\n",
    "train_op=optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())    \n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    saver.restore(sess, '##FIXME##')\n",
    "    for i in range(100):\n",
    "        \n",
    "        loss,y_pred,target_caption,_,ftmap=sess.run([cost,pred,tokenized_caption,train_op,input_feature_map],\n",
    "                                                    feed_dict={lr:train_lr})    \n",
    "\n",
    "        #print loss\n",
    "        if i% 10==0:\n",
    "            print(\"iteration: \",i, \"loss: \",loss)\n",
    "            \n",
    "     \n",
    "    MODEL_NAME='video_4096model_iter'+str(i)\n",
    "    saver.save(sess, MODEL_NAME)\n",
    "    print('Saved network ',MODEL_NAME)\n",
    "    print(\"Done Training\")\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can use the function below to estimate how well the network is able to predict the next word in the caption. You can evaluate a single image and its caption from the last batch using the index of the batch. If you need a hint look [here](#answer3 \"if the batch_size is 4, batch_id may be any value between 0 and 3.\").\n",
    "\n",
    "#### Please note that depending on the status of the neural network at the time it was saved, incomplete, incoherent, and sometimes inappropriate captions can be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prediction(batch_id,batch_size,words,target_catpion,predicted_caption, ftmap):\n",
    "    TARGETSENTENCE=[words[ind] for ind in target_caption[batch_id]]\n",
    "    PREDICTEDSENTENCE=[words[ind] for ind in np.argmax(y_pred[batch_id::batch_size],1)]\n",
    "    VIDEOCLIPNAME=[x for x in file_loader.keys() if np.array_equal(ftmap[batch_id],file_loader[x])]\n",
    "    return TARGETSENTENCE,PREDICTEDSENTENCE, VIDEOCLIPNAME[0]\n",
    "\n",
    "batch_id=##FIXME##\n",
    "t,p,v=eval_prediction(batch_id,batch_size,words,target_caption,y_pred,ftmap)\n",
    "#print('Ground Truth Words')\n",
    "#print(t)\n",
    "print('Predicted Words')\n",
    "print(p)\n",
    "exp_image=ndimage.imread('/data/msvd/frames_224/'+v+'/frame_000003_224.bmp')\n",
    "plt.imshow(exp_image)\n",
    "plt.title('video clip name:'+v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions\n",
    "[1] Can you load your saved network and use it to generate a caption from a validation image like we did in the image captioning section?\n",
    "\n",
    "A npy file of the feature vectors from the validation dataset is stored /data/msvd/val_msvd_mean_fc7.npy. If you need help please look [here](#answer4 \"Replace ##FIXME## with  /data/msvd/val_msvd_mean_fc7.npy\"). \n",
    "\n",
    "\n",
    "Then load the pretrained network that was just created and use it to generate a caption for a validation clip. A video clip can be tested with the VALDATA variable. If you need a hint look [here](#answer5 \"VALDATA represents the index of a clip you are interested in generating a caption for, any value between 0 and 196 can be used. For example you could set this variable to zero, VALDATA=0. The network saved in the previous cell is video_4096model_iter99. The name of the file is used with saver.restore(sess, MODEL_NAME)\").\n",
    "\n",
    "[2] Do you need to calculate the loss or cost when only performing inference?\n",
    "\n",
    "[3] Do you use dropout when performing inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A npy file of all of the mean feature vectors has been created\n",
    "val_loader=np.load('/data/msvd/val_msvd_mean_fc7.npy').tolist()\n",
    "print('Number of videos in the validation set: ', len(val_loader.keys()))\n",
    "#Create 2 lists one of the video_id and the other with feature maps.\n",
    "val_id_key=[]\n",
    "val_maps_to_id=[]\n",
    "\n",
    "for observed_vid in val_loader.keys():   \n",
    "        val_id_key.append([observed_vid])\n",
    "        val_maps_to_id.append(val_loader[observed_vid])\n",
    "  \n",
    "print('number of captions and datapoints',len(val_maps_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size=1\n",
    "num_steps=20\n",
    "print_topn=0 #0for do not display \n",
    "printnum0f=3\n",
    "#Choose a image to caption\n",
    "VALDATA=##FIXME## #Val Video Id\n",
    "\n",
    "variables_dict = {\n",
    "    \"weights_mscoco\":tf.Variable(tf.truncated_normal([num_hidden,dict_length],\n",
    "                                                     stddev=1.0,dtype=tf.float32),name=\"weights_mscoco\"),\n",
    "    \"biases_mscoco\": tf.Variable(tf.truncated_normal([dict_length],\n",
    "                                                     stddev=1.0,dtype=tf.float32), name=\"biases_mscoco\")}\n",
    "\n",
    "\n",
    "StartCaption=np.zeros([batch_size,num_steps],dtype=np.int32).tolist()\n",
    "\n",
    "CaptionPlaceHolder = tf.placeholder(dtype=tf.int32, shape=(batch_size , num_steps))\n",
    "\n",
    "ValFeatureMap=val_maps_to_id[VALDATA]\n",
    "X_one_hot=tf.nn.embedding_lookup(np.identity(dict_length), CaptionPlaceHolder) #[batch,num_steps,dictionary_length][2,6,7]\n",
    "    #ImageFeatureTensor=input_feature_map\n",
    "Xconcat=tf.concat([ValFeatureMap+tf.zeros([num_steps,batch_size,4096]), \n",
    "                            tf.unstack(tf.to_float(X_one_hot),num_steps,1)],2)#[:num_steps,:,:]\n",
    "\n",
    "pred=rnn_model(Xconcat,1.0,1.0,num_layers,num_hidden)\n",
    "pred=tf.nn.softmax(pred)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())    \n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    #Load a pretrained network\n",
    "    saver.restore(sess, '##FIXME##')\n",
    "    print('Model restored from file')\n",
    "    for i in range(num_steps-1):\n",
    "        predict_next_word=sess.run([pred],feed_dict={CaptionPlaceHolder:StartCaption})\n",
    "        INDEX=np.argmax(predict_next_word[0][i])\n",
    "        StartCaption[0][i+1]=INDEX\n",
    "        if print_topn !=0:\n",
    "            print(\"Top \",str(printnum0f), \"predictions for the\", str(i+1), \"word in the predicted caption\" )\n",
    "            result_args = np.argsort(predict_next_word[0][i])[-printnum0f:][::-1]\n",
    "            NextWord=[words[x] for x in result_args]\n",
    "            print(NextWord)\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close() \n",
    "\n",
    "#print(\"ground truth caption: \",val_caption_to_id[VALDATA])\n",
    "v=val_id_key[VALDATA][0]\n",
    "img=ndimage.imread('/data/msvd/frames_224/'+v+'/frame_000003_224.bmp')\n",
    "plt.imshow(img)\n",
    "PredictedCaption=[words[x] for x in StartCaption[0]]\n",
    "\n",
    "print(\"predicted sentence: \",PredictedCaption[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Free our GPU memory before proceeding to the next part of the lab\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References \n",
    "[1] Venugopalan, S., et al. \"Translating videos to natural language using deep recurrent neural networks.\" arXiv preprint arXiv:1412.4729 (2014).\n",
    "\n",
    "[2] Chen, David L., and William B. Dolan. \"Collecting highly parallel data for paraphrase evaluation.\" Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computational Linguistics, 2011.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
